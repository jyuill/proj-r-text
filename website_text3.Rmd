---
title: "Using R to Scrape Website Text, pt 3"
author: "John Yuill"
date: "October 1, 2016"
output: html_document
---

```{r, global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)


```

### Scraping MULTIPLE Website Pages to Extract Text for Analysis

Enhancement over pt 2, with more pages and more analysis. 

* Using rvest package
    + note that XML package also has to be installed
    
```{r}
library(rvest)
library(dplyr)

```

#### Process

(same as pt 2)

1. Select URLs and load them into data frame
2. Set up a loop to process each URL by:
    + scraping the target web page by using rvest's 'read_html' function
    + applying a CSS selector to get target content
    + removing HTM tags with html_text
    + removing any common unwanted content - standard footers, etc, using standard subsetting
    + collapsing all text into single character string
3. Over-write the original data frame with the new data compiled in the loop
4. Analysze the results 

```{r}
pgurls <- c("https://www.needforspeed.com/news/fiveways",
            "https://www.needforspeed.com/news/under-the-hood-7",
            "https://www.needforspeed.com/news/nfs-enters-vault",
            "https://www.needforspeed.com/news/game-update-speedlists",
            "https://www.needforspeed.com/news/origin-access-trial-date",
            "https://www.needforspeed.com/news/game-update-hot-rods",
           "https://www.needforspeed.com/news/gamescom-gameplay",
           "https://www.needforspeed.com/news/five-icons",
           "https://www.needforspeed.com/news/gamescom15-recap",
           "https://www.needforspeed.com/news/the-icons",
           "https://www.needforspeed.com/news/gamescom-trailer",
           "https://www.needforspeed.com/news/deluxe-edition"
            )

# set up data frame to hold page urls - more scalable
articles.df <- data.frame("page"=pgurls)

artloop.df <- data.frame() # temporary data frame to use in loop

for (p in 1:nrow(articles.df)){
  pgurl <- pgurls[p]
  htmlpg <- read_html(pgurl)
  cssnode <- "p" 
  article <- html_nodes(htmlpg,cssnode)
  articletext <- html_text(article)
  articletext <- articletext[1:(length(articletext)-3)]
  articlesingle <- paste(articletext, collapse=" ")
  art.df <- data.frame("page"=pgurl,"text"=articlesingle)
  artloop.df <-rbind(artloop.df,art.df)
}
articles.df <- artloop.df
articles.df <- articles.df %>% mutate(textshort=strtrim(text,140))

```

* Css selector: `r cssnode`

```{r}
articles.df %>% select(page,textshort)
```

#### Count Words in Article

```{r}
# count words in article

for(c in 1:nrow(articles.df)){
  wordcount <- sapply(gregexpr("[[:alpha:]]+", articles.df[c,2]), function(x) sum(x > 0))
  articles.df$words[c] <- wordcount 
}

## testing ideas around buckets
#articles.df <- articles.df %>% mutate(size=if(words<300) "small" else "med")
#articles.df <- articles.df %>% mutate(size=round(words,-2))
```
Word count for each article: 
```{r}
#articles.df[,c(1,3)]
articles.df %>% select(page,words)
```

#### Add Publication Date
```{r}
artloop.df <- data.frame()
for (p in 1:nrow(articles.df)){
  pgurl <- pgurls[p]
  htmlpg <- read_html(pgurl)
  cssnode <- ".article_publish-date" 
  articledate <- html_node(htmlpg,cssnode)
  articledate <- html_text(articledate)
  art.df <- data.frame("page"=pgurl,"pubdate"=articledate)
  artloop.df <-rbind(artloop.df,art.df)
}
articles.df <- left_join(articles.df,artloop.df, by="page")
articles.df$pubdate <- as.Date(articles.df$pubdate, format="%B %d, %Y")

```

Word count and publication date for each article:
```{r}
articles.df %>% select(page,words,pubdate)
```

#### Analysis

Word Count stats
```{r}
meanwords <- mean(articles.df$words)
medwords <- median(articles.df$words)

```

* Mean word count: `r meanwords`
* Median word count: `r medwords`

Histogram
```{r}
library(ggplot2)

ggplot(articles.df, aes(words))+geom_histogram(binwidth = 200)+
  theme_bw()
```

### Additional opportunities
* Could create buckets of word length and load into custom dimension
* Could identify posts with images