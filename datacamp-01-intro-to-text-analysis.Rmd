---
title: 'Datacamp: Intro to Text Analysis in R'
output:
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
---

```{r, include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE)
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud)
```

[Intro to Text Analysis in R](https://learn.datacamp.com/courses/introduction-to-text-analysis-in-r)

Part of Skill Track: [Text Mining with R](https://learn.datacamp.com/skill-tracks/text-mining-with-r)

## Get data

Using c&c twitter data, since don't have access to data from course, which is also twitter data.

```{r, GET DATA}
twitter_data <- read_csv('input/cnc-twitter.csv')
twitter_data$postdate <- mdy(twitter_data$postdate)
```

## Summarize

```{r}
ntweet <- nrow(twitter_data)
ntweeters <- length(unique(twitter_data$screenName))
cat(ntweet,"tweets by",ntweeters,"tweeters \n")
cat("How many retweets? (TRUE)")
table(twitter_data$isRetweet)


```


```{r}
rtweets <- twitter_data %>% count(isRetweet)

```

## Tokenize

Tokenize individual words in tweets

* break out tweets into individual words
* maintains all other cols in data frame
* count occurrences of each word, sort by most to least mentioned

```{r, TOKENIZE}
## ALL WORDS SEPARATELY - each occurence
tweet_token_sep <- twitter_data %>%
  unnest_tokens(word, text)

## Group words by counting occurences and order most to least
tweet_token <- tweet_token_sep %>% 
  count(word) %>% arrange(desc(n))

head(tweet_token, 10)

```

* remove 'stop' words: 'the', 'and', 'of' other meaningless

```{r, STOP WORDS}
## REMOVE STOP WORDS
## stop words are built in to the tidytext pkg
## use anti_join of stop word list with overall word list
tweet_token <- twitter_data %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

## get word counts after removing stop words
tweet_token_all <- tweet_token %>% 
  count(word) %>% arrange(desc(n))
## show top 10
head(tweet_token_all, 10)

```

* can see that 'the', 'and' are no longer present
* example below on how to remove custom stop words

## Retweets vs Non-Retweets

Separate retweets from non-retweets to count words that appear in each and compare.

```{r}
## COUNT FOR NON-RETWEETS
tweet_token_nrt <- tweet_token %>% 
  filter(isRetweet==FALSE) %>%
  count(word) %>% arrange(desc(n))
head(tweet_token_nrt)

## COUNT FOR RETWEETS
tweet_token_rt <- tweet_token %>% 
  filter(isRetweet==TRUE) %>%
  count(word) %>% arrange(desc(n))

head(tweet_token_rt, 10)
```


## Visualize (initial)

* use **x=reorder(word, n)** to ensure words shown in order of mentions 

```{r VIZ WORDS 1}
tweet_chart <- tweet_token_all %>% filter(n>1000) 
ggplot(tweet_chart, aes(x=reorder(word, n), y=n))+geom_col()+
  coord_flip()
```

## Remove custom stop words

Specify stop words that are specific to this data and are not very informative. Can add them to the existing default stop word list for removing.

```{r}
## use tribble to create a data frame of custom stop words
stop_words_cust <- tribble(~word, ~lexicon,
        "https","CUSTOM",
        "rt","CUSTOM",
        "t.co","CUSTOM",
        "amp","CUSTOM"
        )

## join to existing default stop words
stop_words2 <- bind_rows(stop_words, stop_words_cust)

## anti-join with new stop words

tweet_token <- twitter_data %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words2, by='word')

## group by word with counts
tweet_token_all <- tweet_token %>% 
  count(word) %>% arrange(desc(n))

```

## Visualize after clean-up

```{r VIZ WORDS 2}
tweet_chart <- tweet_token_all %>% filter(n>500) 
ggplot(tweet_chart, aes(x=reorder(word, n), y=n))+geom_col()+
  coord_flip()
```

## Compare Retweets vs non-Retweets

Use new token table to split up into RT and not RT and count occurrences of words.

```{r RTs}

tweet_token_nrt <- tweet_token %>% 
  filter(isRetweet==FALSE) %>%
  count(word) %>% arrange(desc(n))
head(tweet_token_nrt)

tweet_token_rt <- tweet_token %>% 
  filter(isRetweet==TRUE) %>%
  count(word) %>% arrange(desc(n))

head(tweet_token_rt, 10)
```

```{r RT col}
## Set columns to identify retweet vs non-retweet
tweet_token_nrt$rt <- FALSE
tweet_token_rt$rt <- TRUE

## Join retweets and non-retweets
tweet_token_rt_all <- bind_rows(tweet_token_nrt, tweet_token_rt)

## reorder for plotting
tweet_token_rt_all <- tweet_token_rt_all %>% mutate(
  word=fct_reorder(word, n)
)
```

## Visualize Word Volume by RT vs non-RT

```{r, VISUALIZE RT}
tweet_chart <- tweet_token_rt_all %>% filter(n>250) 
ggplot(tweet_chart, aes(x=reorder(word, n), y=n, fill=rt))+geom_col(position='dodge')+
  coord_flip()
```

Visualize using faceting

```{r, VISUALIZE FACET}
ggplot(tweet_chart, aes(x=word, y=n, fill=rt))+geom_col()+
  facet_grid(.~rt, scales="free_x")+
  coord_flip()+
  theme(legend.position = 'none')

## alternative way to hide legend
#ggplot(tweet_chart, aes(x=word, y=n, fill=rt))+geom_col(show.legend=FALSE)+
#  facet_grid(.~rt, scales="free_x")+
#  coord_flip()

## facet_wrap instead of facet_grid, using free_y to size spacing in y axis (not sure why reorder doesn't quite work)
ggplot(tweet_chart, aes(x=reorder(word, n), y=n, fill=rt))+geom_col(show.legend=FALSE)+
  facet_wrap(~rt, scales="free_y")+
  coord_flip()



```

### Word Clouds ;P

* for what it is worth...

```{r, WORD CLOUDS}
## word clouds for show with word cloud library

wordcloud(
  words=tweet_token_rt$word,
  freq=tweet_token_rt$n,
  max.words=20,
  color='blue'
)

```

* Attempt to show RT vs non-RT words by color: #fail

```{r}
## attempt identify RT vs nonRT by color - fail

wordcloud(
  words=tweet_token_rt_all$word,
  freq=tweet_token_rt_all$n,
  max.words=20,
  color=tweet_token_rt_all$rt
)
```


## Sentiment Analysis

### Sentiment Dictionaries

tidytext package has 4 sentiment dictionaries:

* Bing
* Afinn
* Loughran
* NRC

Each dictionary was created for particular purposes, so which one will be apply to your data depends on the nature of that data and what you are trying to learn from it.

Bing

```{r}
get_sentiments('bing') %>%
  count(sentiment) %>%
  arrange(desc(n))
```

Afinn

* generates error

```{r, AFINN}
# get_sentiments('afinn') %>%
#   count(sentiment) %>%
#   arrange(desc(n))
```

Loughran

```{r, LOUGHRAN}
get_sentiments('loughran') %>%
  count(sentiment) %>%
  arrange(desc(n))

```

NRC

```{r, NRC}
get_sentiments('nrc') %>%
  count(sentiment) %>%
  arrange(desc(n))
```

```{r}
senti_count <- get_sentiments('nrc') %>%
  count(sentiment) %>%
  arrange(desc(n))

ggplot(senti_count, aes(x=reorder(sentiment, n), y=n))+geom_col()+
  coord_flip()+
  labs(title='NRC Sentiments',
       x='word count',
       y='sentiment')
```

## Get sentiments for data set

```{r}
## joint sentiment dictionary with full data set
## rows will be reduced because not every word matches what is in dictionary
tweet_sentiments <- tweet_token_rt_all %>%
  inner_join(get_sentiments('nrc'))

## count sentiments -> taking into account rt vs nrt
tweet_sentiment_counts <- tweet_sentiments %>% 
  count(sentiment, rt) %>%
  arrange(desc(n,rt))

tweet_sentiment_counts

```

```{r}

```


Get top words for specific sentiments

```{r}
## filter for specified sentiment categories 
tweet_sentiment_spec <- tweet_token %>%
  inner_join(get_sentiments('nrc')) %>%
  filter(sentiment %in% c('positive', 'negative', 'disgust'))

## count top words in each category
tweet_senti_count <- tweet_sentiment_spec %>%
  count(word, sentiment) %>%
  group_by(sentiment) %>% top_n(10, n) %>%
  ungroup() %>%
  mutate(word2=fct_reorder(word,n))

```

## Visualizing words by sentiment

```{r}
ggplot(tweet_senti_count, aes(x=word2, y=n, fill=sentiment))+
  geom_col(show.legend=FALSE)+
  facet_wrap(~sentiment, scales='free')+
  coord_flip()+
  labs(title='top words by sentiment',
       x='words')
```


