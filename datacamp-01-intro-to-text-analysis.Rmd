---
title: 'Datacamp: Intro to Text Analysis in R'
output:
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
---

```{r, include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE)
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud)
```

[Intro to Text Analysis in R](https://learn.datacamp.com/courses/introduction-to-text-analysis-in-r)

Part of Skill Track: [Text Mining with R](https://learn.datacamp.com/skill-tracks/text-mining-with-r)

## Get data

Using c&c twitter data, since don't have access to data from course, which is also twitter data.

```{r, GET DATA}
twitter_data <- read_csv('input/cnc-twitter.csv')
twitter_data$postdate <- mdy(twitter_data$postdate)
```

## Summarize

```{r}
ntweet <- nrow(twitter_data)
ntweeters <- length(unique(twitter_data$screenName))
cat(ntweet,"tweets by",ntweeters,"tweeters \n")
cat("How many retweets? (TRUE)")
table(twitter_data$isRetweet)


```


```{r}
rtweets <- twitter_data %>% count(isRetweet)

```

## Tokenize

Tokenize individual words in tweet

```{r, TOKENIZE}
## ALL WORDS
tweet_token <- twitter_data %>%
  unnest_tokens(word, text)

tweet_token <- tweet_token %>% 
  count(word) %>% arrange(desc(n))

head(tweet_token, 10)

```

```{r, STOP WORDS}
## REMOVE STOP WORDS
## stop words are built in to the tidytext pkg
tweet_token <- twitter_data %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

## get word counts after removing stop words
tweet_token_all <- tweet_token %>% 
  count(word) %>% arrange(desc(n))
head(tweet_token_all, 10)

```

## Retweets vs Non-Retweets

Separate retweets from non-retweets to count words that appear in each and compare.

```{r}
## COUNT FOR NON-RETWEETS
tweet_token_nrt <- tweet_token %>% 
  filter(isRetweet==FALSE) %>%
  count(word) %>% arrange(desc(n))
head(tweet_token_nrt)

## COUNT FOR RETWEETS
tweet_token_rt <- tweet_token %>% 
  filter(isRetweet==TRUE) %>%
  count(word) %>% arrange(desc(n))

head(tweet_token_rt, 10)
```


## Visualize

```{r}
tweet_chart <- tweet_token %>% filter(n>1000) 
ggplot(tweet_chart, aes(x=word, y=n))+geom_col()+
  coord_flip()
```

## Remove custom stop words

Specify stop words that are specific to this data and are not very informative. Can add them to the existing default stop word list for removing.

```{r}
## use tribble to create a data frame of custom stop words
stop_words_cust <- tribble(~word, ~lexicon,
        "conquer","CUSTOM",
        "command","CUSTOM",
        "https","CUSTOM",
        "rt","CUSTOM",
        "t.co","CUSTOM"
        )

## join to existing default stop words
stop_words2 <- bind_rows(stop_words, stop_words_cust)

## anti-join with new stop words

tweet_token <- twitter_data %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words2)

```

Use new token table to split up into RT and not RT and count occurrences of words.

```{r}
tweet_token_all <- tweet_token %>% 
  count(word) %>% arrange(desc(n))
head(tweet_token_all, 10)

tweet_token_nrt <- tweet_token %>% 
  filter(isRetweet==FALSE) %>%
  count(word) %>% arrange(desc(n))
head(tweet_token_nrt)

tweet_token_rt <- tweet_token %>% 
  filter(isRetweet==TRUE) %>%
  count(word) %>% arrange(desc(n))

head(tweet_token_rt, 10)
```

```{r}
## Set columns to identify retweet vs non-retweet
tweet_token_nrt$rt <- FALSE
tweet_token_rt$rt <- TRUE

## Join retweets and non-retweets
tweet_token_rt_all <- bind_rows(tweet_token_nrt, tweet_token_rt)

## reorder for plotting
tweet_token_rt_all <- tweet_token_rt_all %>% mutate(
  word=fct_reorder(word, n)
)
```


```{r, VISUALIZE 2}
tweet_chart <- tweet_token_rt_all %>% filter(n>200) 
ggplot(tweet_chart, aes(x=word, y=n, fill=rt))+geom_col(position='dodge')+
  coord_flip()
```

Visualize using faceting

```{r, VISUALIZE FACET}
ggplot(tweet_chart, aes(x=word, y=n, fill=rt))+geom_col()+
  facet_grid(.~rt, scales="free_x")+
  coord_flip()+
  theme(legend.position = 'none')

## alternative way to hide legend
#ggplot(tweet_chart, aes(x=word, y=n, fill=rt))+geom_col(show.legend=FALSE)+
#  facet_grid(.~rt, scales="free_x")+
#  coord_flip()

## facet_wrap instead of facet_grid, using free_y to size spacing in y axis
ggplot(tweet_chart, aes(x=word, y=n, fill=rt))+geom_col(show.legend=FALSE)+
  facet_wrap(~rt, scales="free_y")+
  coord_flip()



```

## Word Clouds ;P

```{r, WORD CLOUDS}
## word clouds for show with word cloud library

wordcloud(
  words=tweet_token_rt$word,
  freq=tweet_token_rt$n,
  max.words=20,
  color='blue'
)


```

## Sentiment Analysis

### Sentiment Dictionaries

tidytext package has 4 sentiment dictionaries:

* Bing
* Afinn
* Loughran
* NRC

Each dictionary was created for particular purposes, so which one will be apply to your data depends on the nature of that data and what you are trying to learn from it.

Bing

```{r}
get_sentiments('bing') %>%
  count(sentiment) %>%
  arrange(desc(n))
```

Afinn

* generates error

```{r, AFINN}
# get_sentiments('afinn') %>%
#   count(sentiment) %>%
#   arrange(desc(n))
```

Loughran

```{r, LOUGHRAN}
get_sentiments('loughran') %>%
  count(sentiment) %>%
  arrange(desc(n))

```

NRC

```{r, NRC}
get_sentiments('nrc') %>%
  count(sentiment) %>%
  arrange(desc(n))
```

```{r}
senti_count <- get_sentiments('nrc') %>%
  count(sentiment) %>%
  arrange(desc(n))

ggplot(senti_count, aes(x=sentiment, y=n))+geom_col()+
  coord_flip()+
  labs(title='NRC Sentiments',
       x='word count',
       y='sentiment')
```

## Get sentiments for data set

```{r}
## joint sentiment dictionary with full data set
## rows will be reduced because not every word matches what is in dictionary
tweet_sentiments <- tweet_token_rt_all %>%
  inner_join(get_sentiments('nrc'))

## count sentiments -> taking into account rt vs nrt
tweet_sentiment_counts <- tweet_sentiments %>% 
  count(sentiment, rt) %>%
  arrange(desc(n,rt))

tweet_sentiment_counts

```

```{r}

```


Get top words for specific sentiments

```{r}
## filter for specified sentiment categories 
tweet_sentiment_spec <- tweet_token %>%
  inner_join(get_sentiments('nrc')) %>%
  filter(sentiment %in% c('positive', 'negative', 'disgust'))
## count top words in each category
tweet_senti_count <- tweet_sentiment_spec %>%
  count(word, sentiment) %>%
  group_by(sentiment) %>% top_n(10, n) %>%
  ungroup() %>%
  mutate(word2=fct_reorder(word,n))

```

## Visualizing words by sentiment

```{r}
ggplot(tweet_senti_count, aes(x=word2, y=n, fill=sentiment))+
  geom_col(show.legend=FALSE)+
  facet_wrap(~sentiment, scales='free')+
  coord_flip()+
  labs(title='top words by sentiment',
       x='words')
```

