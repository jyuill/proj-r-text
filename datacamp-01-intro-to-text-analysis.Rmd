---
title: 'Datacamp: Intro to Text Analysis in R'
output:
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
---

```{r, include=FALSE}
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE)
library(tidyverse)
library(tidytext)
library(tm) ## needed for dtm
library(topicmodels)
library(lubridate)
library(wordcloud)

theme_set(theme_bw())
```

[Intro to Text Analysis in R](https://learn.datacamp.com/courses/introduction-to-text-analysis-in-r)

Part of Skill Track: [Text Mining with R](https://learn.datacamp.com/skill-tracks/text-mining-with-r)

## Get data

Using c&c twitter data, since don't have access to data from course, which is also twitter data.

```{r, GET DATA}
twitter_data <- read_csv('input/cnc-twitter.csv')
twitter_data$postdate <- mdy(twitter_data$postdate)

## create new tweet id to work around issues with long numbers being corrupted in processing
id_pool <- as.integer(runif(10000, min=100000000, max=999999999)) ## generate numbers
#length(unique(id_pool)) ## check number of uinques
## take sample same size as number of data frame rows, with no replacement to ensure unique
ids <- sample(id_pool, nrow(twitter_data), replace=FALSE)
## create new id col for use later (DTM etc)
twitter_data$twid <- ids

```

## Summarize

```{r}
ntweet <- nrow(twitter_data)
ntweeters <- length(unique(twitter_data$screenName))
cat(ntweet,"tweets by",ntweeters,"tweeters \n")
cat("How many retweets? (TRUE)")
table(twitter_data$isRetweet)


```


```{r}
rtweets <- twitter_data %>% count(isRetweet)

```

## Tokenize

Tokenize individual words in tweets

* break out tweets into individual words
* maintains all other cols in data frame
* count occurrences of each word, sort by most to least mentioned

```{r, TOKENIZE}
## ALL WORDS SEPARATELY - each occurence
tweet_token_sep <- twitter_data %>%
  unnest_tokens(word, text)

## Group words by counting occurences and order most to least
tweet_token <- tweet_token_sep %>% 
  count(word) %>% arrange(desc(n))

head(tweet_token, 10)

```

* remove 'stop' words: 'the', 'and', 'of' other meaningless

```{r, STOP WORDS}
## REMOVE STOP WORDS
## stop words are built in to the tidytext pkg
## use anti_join of stop word list with overall word list
tweet_token <- twitter_data %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

## get word counts after removing stop words
tweet_token_all <- tweet_token %>% 
  count(word) %>% arrange(desc(n))
## show top 10
head(tweet_token_all, 10)

```

* can see that 'the', 'and' are no longer present
* example below on how to remove custom stop words

## Retweets vs Non-Retweets

Separate retweets from non-retweets to count words that appear in each and compare.

```{r}
## COUNT FOR NON-RETWEETS
tweet_token_nrt <- tweet_token %>% 
  filter(isRetweet==FALSE) %>%
  count(word) %>% arrange(desc(n))
head(tweet_token_nrt)

## COUNT FOR RETWEETS
tweet_token_rt <- tweet_token %>% 
  filter(isRetweet==TRUE) %>%
  count(word) %>% arrange(desc(n))

head(tweet_token_rt, 10)
```


## Visualize (initial)

* use **x=reorder(word, n)** to ensure words shown in order of mentions 

```{r VIZ WORDS 1}
tweet_chart <- tweet_token_all %>% filter(n>1000) 
ggplot(tweet_chart, aes(x=reorder(word, n), y=n))+geom_col()+
  coord_flip()
```

## Remove custom stop words

Specify stop words that are specific to this data and are not very informative. Can add them to the existing default stop word list for removing.

```{r}
## use tribble to create a data frame of custom stop words
stop_words_cust <- tribble(~word, ~lexicon,
        "https","CUSTOM",
        "rt","CUSTOM",
        "t.co","CUSTOM",
        "amp","CUSTOM"
        )

## join to existing default stop words
stop_words2 <- bind_rows(stop_words, stop_words_cust)

## anti-join with new stop words

tweet_token <- twitter_data %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words2, by='word')

## group by word with counts
tweet_token_all <- tweet_token %>% 
  count(word) %>% arrange(desc(n))

```

## Visualize after clean-up

```{r VIZ WORDS 2}
tweet_chart <- tweet_token_all %>% filter(n>500) 
ggplot(tweet_chart, aes(x=reorder(word, n), y=n))+geom_col()+
  coord_flip()
```

## Compare Retweets vs non-Retweets

Use new token table to split up into RT and not RT and count occurrences of words.

```{r RTs}

tweet_token_nrt <- tweet_token %>% 
  filter(isRetweet==FALSE) %>%
  count(word) %>% arrange(desc(n))
head(tweet_token_nrt)

tweet_token_rt <- tweet_token %>% 
  filter(isRetweet==TRUE) %>%
  count(word) %>% arrange(desc(n))

head(tweet_token_rt, 10)
```

```{r RT col}
## Set columns to identify retweet vs non-retweet
tweet_token_nrt$rt <- FALSE
tweet_token_rt$rt <- TRUE

## Join retweets and non-retweets
tweet_token_rt_all <- bind_rows(tweet_token_nrt, tweet_token_rt)

## reorder for plotting
tweet_token_rt_all <- tweet_token_rt_all %>% mutate(
  word=fct_reorder(word, n)
)
```

## Visualize Word Volume by RT vs non-RT

```{r, VISUALIZE RT}
tweet_chart <- tweet_token_rt_all %>% filter(n>250) 
ggplot(tweet_chart, aes(x=reorder(word, n), y=n, fill=rt))+geom_col(position='dodge')+
  coord_flip()
```

Visualize using faceting

```{r, VISUALIZE FACET}
ggplot(tweet_chart, aes(x=word, y=n, fill=rt))+geom_col()+
  facet_grid(.~rt, scales="free_x")+
  coord_flip()+
  theme(legend.position = 'none')

## alternative way to hide legend
#ggplot(tweet_chart, aes(x=word, y=n, fill=rt))+geom_col(show.legend=FALSE)+
#  facet_grid(.~rt, scales="free_x")+
#  coord_flip()

## facet_wrap instead of facet_grid, using free_y to size spacing in y axis (not sure why reorder doesn't quite work)
ggplot(tweet_chart, aes(x=reorder(word, n), y=n, fill=rt))+geom_col(show.legend=FALSE)+
  facet_wrap(~rt, scales="free_y")+
  coord_flip()



```

### Word Clouds ;P

* for what it is worth...

```{r, WORD CLOUDS}
## word clouds for show with word cloud library

wordcloud(
  words=tweet_token_rt$word,
  freq=tweet_token_rt$n,
  max.words=20,
  color='blue'
)

```

* Attempt to show RT vs non-RT words by color: #fail

```{r}
## attempt identify RT vs nonRT by color - fail

wordcloud(
  words=tweet_token_rt_all$word,
  freq=tweet_token_rt_all$n,
  max.words=20,
  color=tweet_token_rt_all$rt
)
```


## Sentiment Analysis

### Sentiment Dictionaries

tidytext package has 4 sentiment dictionaries:

* Bing
* Afinn
* Loughran
* NRC

Each dictionary was created for particular purposes, so which one will be apply to your data depends on the nature of that data and what you are trying to learn from it.

A look at dictionaries based on sentiment categories and number of terms in each:

(obvious limitation being that sentiment is assigned to individual words irrespective of context or combination)

Bing

```{r}
get_sentiments('bing') %>%
  count(sentiment) %>%
  arrange(desc(n))
```

Afinn

* generates error

```{r, AFINN}
# get_sentiments('afinn') %>%
#   count(sentiment) %>%
#   arrange(desc(n))
```

Loughran

```{r, LOUGHRAN}
get_sentiments('loughran') %>%
  count(sentiment) %>%
  arrange(desc(n))

```

NRC

```{r, NRC}
get_sentiments('nrc') %>%
  count(sentiment) %>%
  arrange(desc(n))
```

```{r}
senti_count <- get_sentiments('nrc') %>%
  count(sentiment) %>%
  arrange(desc(n))

ggplot(senti_count, aes(x=reorder(sentiment, n), y=n))+geom_col()+
  coord_flip()+
  labs(title='NRC Sentiments',
       x='word count',
       y='sentiment')
```

## Get sentiments for data set

```{r}
## joint sentiment dictionary with full data set
## rows will be reduced because not every word matches what is in dictionary
tweet_sentiments <- tweet_token_rt_all %>%
  inner_join(get_sentiments('nrc'))

## count sentiments -> taking into account rt vs nrt
tweet_sentiment_counts <- tweet_sentiments %>% 
  count(sentiment, rt) %>%
  arrange(desc(n,rt))

tweet_sentiment_counts

```

```{r}

```


Get top words for specific sentiments

```{r}
## filter for specified sentiment categories 
tweet_sentiment_spec <- tweet_token %>%
  inner_join(get_sentiments('nrc')) %>%
  filter(sentiment %in% c('positive', 'negative', 'disgust'))

## count top words in each category
tweet_senti_count <- tweet_sentiment_spec %>%
  count(word, sentiment) %>%
  group_by(sentiment) %>% top_n(10, n) %>%
  ungroup() %>%
  mutate(word2=fct_reorder(word,n))

```

## Visualizing words by sentiment

```{r}
ggplot(tweet_senti_count, aes(x=word2, y=n, fill=sentiment))+
  geom_col(show.legend=FALSE)+
  facet_wrap(~sentiment, scales='free')+
  coord_flip()+
  labs(title='top words by sentiment',
       x='words')
```

## Topic Modeling with LDA

Latent Dirichlet allocation is used to go beyond individual works and understanding topics being referred to, and the sentiment associated with them.

Each 'topic' is a collection of word probabilities for all of the unique words used in the corpus. In the case of twitter analysis, each tweet is its own document and the beta column contains the word probabilities.

Unsupervised learning so it identifies words with highest probability of being grouped together and buckets these into a number of topic buckets, without identifying what the topic is. So it's not saying 'these are the topics' it's saying 'it looks like there is a topic with these words associated with it and another topic with these other words associated with it.' From there, you can figure out what the topics seem to be.

### DTM

Starting point is Document Term Matrix (DTM)

```{r}
## go back to original data and clean up, (stop_words2 is standard + custom from above)
tweet_tok <- tweet_token_sep %>% select(twid, word) %>% anti_join(stop_words2)
## create dtm object
tweet_dtm <- tweet_tok %>% select(twid, word) %>% count(word, twid) %>%
  cast_dtm(twid, word, n)
tweet_dtm
```

```{r}
## create matrix from DTM
tweet_matrix <- as.matrix(tweet_dtm)
## view a sample
tweet_matrix[100:105,2000:2005]
```

### Run Topic Model

#### Two Topics

```{r}
## run model to group terms into topics based on probability of occurenec

lda_out <- LDA(
  tweet_dtm,
  k=2, ## set k to determine how many topics
  method='Gibbs',
  control=list(seed=42)
)
```

```{r}
## check out the structure
glimpse(lda_out)
```

```{r}
## organize into tidy format
lda_topics <- lda_out %>%
  tidy(matrix='beta')

lda_topics %>% arrange(desc(beta))
```

```{r}
## further org for prep for viz
lda_topics_review <- lda_topics %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  mutate(term2=fct_reorder(term, beta))
## visualize
ggplot(lda_topics_review, aes(x=term2, y=beta, fill=as.factor(topic)))+geom_col(show.legend=FALSE)+
  facet_wrap(~topic, scales='free')+ ## make sure scales are free
  coord_flip()
```

#### Three Topics

```{r}
## may take some time depending on data size etc
## specify number of topics
knum <- 3

## run model to group words by topic
lda_out <- LDA(
  tweet_dtm,
  k=knum,
  method='Gibbs',
  control=list(seed=42)
)

## organize data in tidy format
lda_topics <- lda_out %>%
  tidy(matrix='beta')

lda_topics %>% arrange(desc(beta))
## further organization as prep for viz
lda_topics_review <- lda_topics %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  mutate(term2=fct_reorder(term, beta))
## visualize top terms by topic to help identify topics
ggplot(lda_topics_review, aes(x=term2, y=beta, fill=as.factor(topic)))+geom_col(show.legend=FALSE)+
  facet_wrap(~topic, scales='free')+
  coord_flip()
```

#### Four Topics

```{r}
## may take some time depending on data size etc
## specify number of topics
knum <- 4

## run model to group words by topic
lda_out <- LDA(
  tweet_dtm,
  k=knum,
  method='Gibbs',
  control=list(seed=42)
)

## organize data in tidy format
lda_topics <- lda_out %>%
  tidy(matrix='beta')

lda_topics %>% arrange(desc(beta))
## further organization as prep for viz
lda_topics_review <- lda_topics %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  mutate(term2=fct_reorder(term, beta))
## visualize top terms by topic to help identify topics
ggplot(lda_topics_review, aes(x=term2, y=beta, fill=as.factor(topic)))+geom_col(show.legend=FALSE)+
  facet_wrap(~topic, scales='free')+
  coord_flip()
```

#### Art of Topic Selection

Determining topics is subjective process.

* keep adding topics as long as they are coming up different
* if start seeing repeat topics/terms, have gone too far
* name the topics based on combination of high probability words

## Further Resources

DataCamp courses:

* Sentiment Analysis in R: The Tidy Way https://learn.datacamp.com/courses/sentiment-analysis-in-r 
* Topic Modeling in R https://learn.datacamp.com/courses/topic-modeling-in-r 

Book

* Text Mining with R (David Robinson, et al) https://www.tidytextmining.com/
