---
title: "Using R to Scrape Website Text, pt 3: NFS Example"
author: "John Yuill"
date: "October 1, 2016"
output: html_document
---
Note: this example no longer works because the URLs used are no longer active (they redirect to a more recent Need for Speed site). The code will run but the data will be meaningless.

* 'website_test3-nfs.html' preserves results from the original code

```{r, global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)

```

### Scraping MULTIPLE Website Pages to Extract Text for Analysis

Enhancement over pt 2, with more pages and more analysis. 

* Using rvest package
    + note that XML package also has to be installed
    
```{r}
library(rvest)
library(dplyr)

```

#### Process

(same as pt 2)

1. Select URLs and load them into data frame
2. Set up a loop to process each URL by:
    + scraping the target web page by using rvest's 'read_html' function
    + applying a CSS selector to get target content
    + removing HTM tags with html_text
    + removing any common unwanted content - standard footers, etc, using standard subsetting
    + collapsing all text into single character string
3. Over-write the original data frame with the new data compiled in the loop
4. Analysze the results 

```{r}
pgurls <- c("https://www.needforspeed.com/news/fiveways",
            "https://www.needforspeed.com/news/under-the-hood-7",
            "https://www.needforspeed.com/news/nfs-enters-vault",
            "https://www.needforspeed.com/news/game-update-speedlists",
            "https://www.needforspeed.com/news/origin-access-trial-date",
            "https://www.needforspeed.com/news/game-update-hot-rods",
           "https://www.needforspeed.com/news/gamescom-gameplay",
           "https://www.needforspeed.com/news/five-icons",
           "https://www.needforspeed.com/news/gamescom15-recap",
           "https://www.needforspeed.com/news/the-icons",
           "https://www.needforspeed.com/news/gamescom-trailer",
           "https://www.needforspeed.com/news/deluxe-edition"
            )

# set up data frame to hold page urls - more scalable
articles.df <- data.frame("page"=pgurls)

artloop.df <- data.frame() # temporary data frame to use in loop

for (p in 1:nrow(articles.df)){
  pgurl <- pgurls[p]
  htmlpg <- read_html(pgurl)
  cssnode <- "p" 
  article <- html_nodes(htmlpg,cssnode)
  articletext <- html_text(article)
  articletext <- articletext[1:(length(articletext)-3)]
  articlesingle <- paste(articletext, collapse=" ")
  cssnode <- ".article_publish-date" 
  articledate <- html_node(htmlpg,cssnode)
  articledate <- html_text(articledate)
  art.df <- data.frame("page"=pgurl,"text"=articlesingle,"pubdate"=articledate)
  artloop.df <-rbind(artloop.df,art.df)
}
articles.df <- artloop.df
articles.df$page <- sub("https://","",articles.df$page)
articles.df <- articles.df %>% mutate(textshort=strtrim(text,20))
articles.df$pubdate <- as.Date(articles.df$pubdate, format="%B %d, %Y")
```

* Css selector: `r cssnode`

```{r}
articles.df %>% select(page,textshort)
```

#### Count Words in Article

```{r}
# count words in article

for(c in 1:nrow(articles.df)){
  wordcount <- sapply(gregexpr("[[:alpha:]]+", articles.df[c,2]), function(x) sum(x > 0))
  articles.df$words[c] <- wordcount 
}
```
Publication date and word count for each article: 
```{r}
#articles.df[,c(1,3)]
articles.df %>% select(page,pubdate,words)
```

Set buckets based on word count:

```{r}
## testing ideas around buckets
articles.df <- articles.df %>% mutate(size=ifelse(words<250,"small",
                                                  ifelse(words<500,"med","lg")))
articles.df %>% select(page,words,size)

```

#### Analysis

Word Count stats
```{r}
meanwords <- mean(articles.df$words)
medwords <- median(articles.df$words)

```

* Mean word count: `r meanwords`
* Median word count: `r medwords`

Histogram
```{r}
library(ggplot2)

ggplot(articles.df, aes(words))+geom_histogram(binwidth = 200)+
  theme_bw()
```

### Additional opportunities
* Could identify posts with images - using count of 'img'